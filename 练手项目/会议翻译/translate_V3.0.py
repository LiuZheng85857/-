import sys
import time
import audioop
import pyaudio
import queue
import os
import tempfile
import wave
import numpy as np
import torch
import whisper  # æ ¸å¿ƒåº“
from googletrans import Translator
from PyQt5.QtWidgets import (QApplication, QMainWindow, QWidget, QVBoxLayout, 
                             QHBoxLayout, QLabel, QComboBox, QPushButton, 
                             QTextEdit, QProgressBar, QSplitter, QMessageBox)
from PyQt5.QtCore import Qt, QThread, pyqtSignal

# ================= é…ç½®å‚æ•° =================
# éº¦å…‹é£é‡‡æ ·é…ç½®
CHUNK = 1024 
FORMAT = pyaudio.paInt16 
CHANNELS = 1 
RATE = 16000  # Whisper æœ€ä½³é‡‡æ ·ç‡æ˜¯ 16000Hz
SILENCE_THRESHOLD = 500   
PAUSE_LIMIT = 1.0  # åœé¡¿å¤šä¹…ç®—ä¸€å¥ (Whisper éœ€è¦è¾ƒå®Œæ•´çš„å¥å­ï¼Œå»ºè®®è®¾é•¿ä¸€ç‚¹)

# æ¨¡å‹é€‰æ‹©: "tiny", "base", "small", "medium", "large"
# base: é€Ÿåº¦å¿«ï¼Œå‡†ç¡®ç‡å°šå¯ (æ¨è CPU ä½¿ç”¨)
# small: å‡†ç¡®ç‡é«˜ï¼Œé€Ÿåº¦ç¨æ…¢ (æ¨è GPU ä½¿ç”¨)
MODEL_SIZE = "base" 

# ================= æ ·å¼è¡¨ =================
STYLESHEET = """
QMainWindow { background-color: #f5f6fa; }
QLabel { font-family: "Microsoft YaHei"; font-size: 14px; color: #2f3640; }
QComboBox { padding: 8px; border: 1px solid #dcdde1; border-radius: 5px; background-color: white; }
QPushButton { padding: 10px 20px; border-radius: 6px; font-family: "Microsoft YaHei"; font-weight: bold; color: white; border: none; }
QPushButton#btnStart { background-color: #4cd137; }
QPushButton#btnStart:hover { background-color: #44bd32; }
QPushButton#btnStart:disabled { background-color: #b2bec3; }
QPushButton#btnStop { background-color: #e84118; }
QPushButton#btnStop:hover { background-color: #c23616; }
QPushButton#btnStop:disabled { background-color: #b2bec3; }
QTextEdit { border: 1px solid #dcdde1; border-radius: 6px; background-color: white; padding: 10px; font-size: 14px; }
QProgressBar { border: 1px solid #dcdde1; border-radius: 5px; text-align: center; background-color: #ffffff; height: 15px; }
QProgressBar::chunk { background-color: #00a8ff; border-radius: 5px; }
"""

# ================= éŸ³é¢‘å½•åˆ¶çº¿ç¨‹ =================
class AudioRecorderThread(QThread):
    sig_volume = pyqtSignal(int)
    sig_status = pyqtSignal(str)
    sig_audio_file = pyqtSignal(str) # å‘é€ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    
    def __init__(self, mic_index):
        super().__init__()
        self.mic_index = mic_index
        self.is_running = True
        self.p = pyaudio.PyAudio()
        self.energy_threshold = SILENCE_THRESHOLD

    def run(self):
        stream = None
        try:
            stream = self.p.open(format=FORMAT, channels=CHANNELS, rate=RATE,
                                 input=True, input_device_index=self.mic_index,
                                 frames_per_buffer=CHUNK)
            
            self.sig_status.emit("æ­£åœ¨æ ¡å‡†ç¯å¢ƒå™ªéŸ³...")
            
            # æ ¡å‡†
            temp_energy = []
            for _ in range(30): # ç¨å¾®ä¹…ä¸€ç‚¹
                if not self.is_running: break
                data = stream.read(CHUNK, exception_on_overflow=False)
                rms = audioop.rms(data, 2)
                temp_energy.append(rms)
                time.sleep(0.02)
            
            if temp_energy:
                avg = sum(temp_energy) / len(temp_energy)
                self.energy_threshold = max(avg * 1.5, 300)
                self.sig_status.emit(f"å°±ç»ª (AIæ¨¡å‹åŠ è½½ä¸­...)")

            frames = []
            silent_chunks = 0
            has_speech = False
            max_silent_chunks = int(PAUSE_LIMIT * (RATE / CHUNK))
            
            while self.is_running:
                data = stream.read(CHUNK, exception_on_overflow=False)
                rms = audioop.rms(data, 2)
                level = min(int(rms / 100), 100)
                self.sig_volume.emit(level)
                
                if rms > self.energy_threshold:
                    has_speech = True
                    silent_chunks = 0
                    frames.append(data)
                else:
                    if has_speech:
                        frames.append(data)
                        silent_chunks += 1
                        if silent_chunks > max_silent_chunks:
                            # å¥å°¾æ£€æµ‹ï¼šä¿å­˜ä¸ºä¸´æ—¶wavæ–‡ä»¶
                            self.save_and_send(frames)
                            frames = []
                            has_speech = False
                            silent_chunks = 0
                    else:
                        if len(frames) > 10: frames.pop(0)
                        frames.append(data)

        except Exception as e:
            self.sig_status.emit(f"éº¦å…‹é£é”™è¯¯: {e}")
        finally:
            if stream: stream.stop_stream(); stream.close()
            self.p.terminate()

    def save_and_send(self, frames):
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            fd, path = tempfile.mkstemp(suffix=".wav")
            os.close(fd)
            
            with wave.open(path, 'wb') as wf:
                wf.setnchannels(CHANNELS)
                wf.setsampwidth(self.p.get_sample_size(FORMAT))
                wf.setframerate(RATE)
                wf.writeframes(b''.join(frames))
            
            self.sig_status.emit("æ­£åœ¨ AI è¯†åˆ«ä¸­...")
            self.sig_audio_file.emit(path)
        except Exception as e:
            print(f"File Error: {e}")

    def stop(self):
        self.is_running = False
        self.wait()

# ================= AI è¯†åˆ«ä¸ç¿»è¯‘çº¿ç¨‹ (æ ¸å¿ƒ) =================
class WhisperWorker(QThread):
    sig_result = pyqtSignal(str, str) # jp, cn
    sig_status = pyqtSignal(str)      # ä¸“é—¨ç”¨äºå›ä¼ â€œåŠ è½½å®Œæˆâ€çŠ¶æ€

    def __init__(self):
        super().__init__()
        self.queue = queue.Queue()
        self.is_running = True
        self.model = None
        self.translator = Translator()

    def add_task(self, file_path):
        self.queue.put(file_path)

    def run(self):
        # 1. çº¿ç¨‹å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡ï¼Œè€—æ—¶)
        if self.model is None:
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ˜¾å¡
                device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"æ­£åœ¨åŠ è½½ Whisper æ¨¡å‹ ({MODEL_SIZE}) åˆ° {device}...")
                self.model = whisper.load_model(MODEL_SIZE, device=device)
                self.sig_status.emit("âœ… AI æ¨¡å‹åŠ è½½å®Œæˆï¼Œè¯·å¼€å§‹è¯´è¯")
            except Exception as e:
                self.sig_status.emit(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                return

        while self.is_running:
            try:
                file_path = self.queue.get(timeout=1)
            except queue.Empty:
                continue

            try:
                # 2. è°ƒç”¨ Whisper è¿›è¡Œè¯†åˆ«
                # initial_prompt æœ‰åŠ©äºå¼•å¯¼æ¨¡å‹è¯†åˆ«ä¸ºæ—¥è¯­
                result = self.model.transcribe(file_path, language="ja", fp16=False) 
                jp_text = result["text"].strip()
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                try: os.remove(file_path)
                except: pass

                if not jp_text: continue

                # 3. ç¿»è¯‘ (æ—¥ -> ä¸­)
                # æ³¨ï¼šWhisper å…¶å®ä¹Ÿå¯ä»¥ç›´æ¥ translate taskï¼Œä½†é‚£æ˜¯è½¬è‹±æ–‡ã€‚
                # è¿™é‡Œæˆ‘ä»¬ç”¨ googletrans ç¿»è¯‘è¯†åˆ«å‡ºæ¥çš„æ—¥è¯­æ–‡æœ¬ï¼Œæˆ–è€…ä½ å¯ä»¥å†æ¬¡ç”¨ deepL
                trans_res = self.translator.translate(jp_text, src='ja', dest='zh-cn')
                cn_text = trans_res.text
                
                self.sig_result.emit(jp_text, cn_text)
                
            except Exception as e:
                print(f"Transcribe Error: {e}")

    def stop(self):
        self.is_running = False
        self.wait()

# ================= ä¸»çª—å£ (ä¿æŒä¸å˜) =================
class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("AI æ™ºèƒ½ä¼šè®®ç¿»è¯‘ (Whisper + PyQt5)")
        self.resize(750, 850)
        self.setStyleSheet(STYLESHEET)
        
        self.audio_thread = None
        self.whisper_thread = None
        
        self.init_ui()
        self.refresh_mics()

    def init_ui(self):
        main = QWidget()
        self.setCentralWidget(main)
        layout = QVBoxLayout(main)
        
        # Top
        top = QHBoxLayout()
        top.addWidget(QLabel("ğŸ¤ éº¦å…‹é£:"))
        self.combo_mics = QComboBox()
        self.combo_mics.currentIndexChanged.connect(self.on_mic_change_preview)
        top.addWidget(self.combo_mics, 1)
        btn_ref = QPushButton("åˆ·æ–°")
        btn_ref.clicked.connect(self.refresh_mics)
        top.addWidget(btn_ref)
        layout.addLayout(top)
        
        # Vol
        layout.addWidget(QLabel("å®æ—¶éŸ³é‡:"))
        self.pb_vol = QProgressBar()
        self.pb_vol.setRange(0, 100)
        self.pb_vol.setValue(0)
        layout.addWidget(self.pb_vol)
        
        # Buttons
        h = QHBoxLayout()
        self.btn_start = QPushButton("å¯åŠ¨ AI ç¿»è¯‘")
        self.btn_start.setObjectName("btnStart")
        self.btn_start.clicked.connect(self.start_app)
        
        self.btn_stop = QPushButton("åœæ­¢")
        self.btn_stop.setObjectName("btnStop")
        self.btn_stop.setEnabled(False)
        self.btn_stop.clicked.connect(self.stop_app)
        h.addWidget(self.btn_start)
        h.addWidget(self.btn_stop)
        layout.addLayout(h)
        
        # Text
        splitter = QSplitter(Qt.Vertical)
        self.txt_jp = QTextEdit(); self.txt_jp.setPlaceholderText("AI æ­£åœ¨å‡†å¤‡ä¸­..."); self.txt_jp.setReadOnly(True)
        self.txt_cn = QTextEdit(); self.txt_cn.setPlaceholderText("ç¿»è¯‘å°†æ˜¾ç¤ºåœ¨è¿™é‡Œ..."); self.txt_cn.setReadOnly(True)
        self.txt_cn.setStyleSheet("color: blue; font-weight: bold; font-size: 16px;")
        splitter.addWidget(self.txt_jp)
        splitter.addWidget(self.txt_cn)
        layout.addWidget(splitter)
        
        self.lbl_status = QLabel("å°±ç»ª")
        self.statusBar().addWidget(self.lbl_status)
        
        self.preview_thread = None
        self.start_preview()

    def refresh_mics(self):
        self.combo_mics.blockSignals(True)
        self.combo_mics.clear()
        try:
            p = pyaudio.PyAudio()
            info = p.get_host_api_info_by_index(0)
            numdevices = info.get('deviceCount')
            for i in range(0, numdevices):
                if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:
                    name = p.get_device_info_by_host_api_device_index(0, i).get('name')
                    self.combo_mics.addItem(f"{i}: {name}", i)
            p.terminate()
        except: pass
        self.combo_mics.blockSignals(False)
        self.start_preview()

    def start_preview(self):
        self.stop_preview()
        if self.combo_mics.count() > 0:
            idx = self.combo_mics.currentData()
            if idx is not None:
                self.preview_thread = AudioRecorderThread(idx)
                self.preview_thread.sig_volume.connect(self.pb_vol.setValue)
                self.preview_thread.start()

    def stop_preview(self):
        if self.preview_thread:
            self.preview_thread.stop()
            self.preview_thread = None
            self.pb_vol.setValue(0)

    def on_mic_change_preview(self):
        if self.btn_start.isEnabled():
            self.start_preview()

    def start_app(self):
        idx = self.combo_mics.currentData()
        if idx is None: return
        self.stop_preview()
        
        # 1. å¯åŠ¨ Whisper çº¿ç¨‹
        self.lbl_status.setText("æ­£åœ¨åŠ è½½ AI æ¨¡å‹ (é¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
        self.whisper_thread = WhisperWorker()
        self.whisper_thread.sig_result.connect(self.update_text)
        self.whisper_thread.sig_status.connect(self.on_model_loaded) # ç›‘å¬æ¨¡å‹åŠ è½½çŠ¶æ€
        self.whisper_thread.start()
        
        # 2. å¯åŠ¨å½•éŸ³çº¿ç¨‹
        self.audio_thread = AudioRecorderThread(idx)
        self.audio_thread.sig_volume.connect(self.pb_vol.setValue)
        self.audio_thread.sig_status.connect(self.update_status_safe)
        self.audio_thread.sig_audio_file.connect(self.whisper_thread.add_task) # ä¼ é€’æ–‡ä»¶è·¯å¾„
        self.audio_thread.start()
        
        self.btn_start.setEnabled(False)
        self.btn_stop.setEnabled(True)
        self.combo_mics.setEnabled(False)

    def on_model_loaded(self, msg):
        self.lbl_status.setText(msg)

    def update_status_safe(self, msg):
        # åªæœ‰å½“æ¨¡å‹å·²ç»åŠ è½½å®Œæ¯•ï¼Œå½•éŸ³çº¿ç¨‹çš„çŠ¶æ€æ‰è¦†ç›–æ˜¾ç¤º
        if "åŠ è½½" not in self.lbl_status.text():
             self.lbl_status.setText(msg)

    def stop_app(self):
        if self.audio_thread: self.audio_thread.stop()
        if self.whisper_thread: self.whisper_thread.stop()
        
        self.btn_start.setEnabled(True)
        self.btn_stop.setEnabled(False)
        self.combo_mics.setEnabled(True)
        self.lbl_status.setText("å·²åœæ­¢")
        self.start_preview()

    def update_text(self, jp, cn):
        self.txt_jp.append(jp)
        self.txt_cn.append(cn)
        self.txt_jp.verticalScrollBar().setValue(self.txt_jp.verticalScrollBar().maximum())
        self.txt_cn.verticalScrollBar().setValue(self.txt_cn.verticalScrollBar().maximum())
        self.lbl_status.setText("ã€ç›‘å¬ä¸­ã€‘AI å‡†å¤‡å°±ç»ª...")

    def closeEvent(self, e):
        self.stop_preview()
        self.stop_app()
        e.accept()

if __name__ == "__main__":
    app = QApplication(sys.argv)
    w = MainWindow()
    w.show()
    sys.exit(app.exec_())